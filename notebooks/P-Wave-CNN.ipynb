{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=21, padding=10)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=15, padding=7)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=11, padding=5)\n",
    "        \n",
    "        self.batchnorm32 = nn.BatchNorm1d(num_features=32)\n",
    "        self.batchnorm64 = nn.BatchNorm1d(num_features=64)\n",
    "        self.batchnorm128 = nn.BatchNorm1d(num_features=128)\n",
    "        self.batchnorm512 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4736, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm32(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm64(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm128(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten input for fully connected layers\n",
    "        x = x.view(x.shape[0], -1) \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "traindata = h5py.File(\"../train/scsn_p_2000_2017_6sec_0.5r_pick_train_mix.hdf5\")\n",
    "testdata = h5py.File(\"../test/scsn_p_2000_2017_6sec_0.5r_pick_test_mix.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "train_size = 10000\n",
    "# val_size = train_size *\n",
    "test_size = 2000\n",
    "\n",
    "# Load test data\n",
    "trainset = traindata[\"X\"][:train_size]\n",
    "trainlabels = traindata[\"pwave\"][:train_size]\n",
    "\n",
    "trainset = list(zip(trainset, trainlabels))\n",
    "\n",
    "testset = testdata[\"X\"][:test_size]\n",
    "testlabels = testdata[\"pwave\"][:test_size]\n",
    "\n",
    "testset = list(zip(testset, testlabels))\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Multi-GPU Support to Model \n",
    "In order to run the model on multiple GPU's, we can use the nn.DataParellel method. This method requires that we move all tensors to the cuda:0 (the default gpu) before we can pass them through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(model):\n",
    "    device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CNN(\n",
       "    (conv1): Conv1d(1, 32, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (conv2): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (conv3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (batchnorm32): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm64): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm128): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm512): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=4736, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc3): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parallelize(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function and Optimizer\n",
    "Here we define the loss function and optimizer. For the loss function (criterion), we use the binary cross entropy with logits loss (BCEWithLogitsLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/   30] | train loss: 0.1216 | validation loss: 2.2347\n",
      "Epoch [    2/   30] | train loss: 0.0525 | validation loss: 2.6065\n",
      "Epoch [    3/   30] | train loss: 0.0416 | validation loss: 2.7682\n",
      "Epoch [    4/   30] | train loss: 0.0309 | validation loss: 3.1524\n",
      "Epoch [    5/   30] | train loss: 0.0250 | validation loss: 3.2717\n",
      "Epoch [    6/   30] | train loss: 0.0175 | validation loss: 3.5634\n",
      "Epoch [    7/   30] | train loss: 0.0124 | validation loss: 3.6730\n",
      "Epoch [    8/   30] | train loss: 0.0140 | validation loss: 4.0407\n",
      "Epoch [    9/   30] | train loss: 0.0110 | validation loss: 3.8949\n",
      "Epoch [   10/   30] | train loss: 0.0075 | validation loss: 4.0702\n",
      "Epoch [   11/   30] | train loss: 0.0041 | validation loss: 4.1698\n",
      "Epoch [   12/   30] | train loss: 0.0070 | validation loss: 4.2465\n",
      "Epoch [   13/   30] | train loss: 0.0073 | validation loss: 4.6131\n",
      "Epoch [   14/   30] | train loss: 0.0068 | validation loss: 4.5385\n",
      "Epoch [   15/   30] | train loss: 0.0015 | validation loss: 4.6619\n",
      "Epoch [   16/   30] | train loss: 0.0008 | validation loss: 4.8823\n",
      "Epoch [   17/   30] | train loss: 0.0008 | validation loss: 4.7758\n",
      "Epoch [   18/   30] | train loss: 0.0056 | validation loss: 4.3736\n",
      "Epoch [   19/   30] | train loss: 0.0025 | validation loss: 5.0789\n",
      "Epoch [   20/   30] | train loss: 0.0028 | validation loss: 5.3408\n",
      "Epoch [   21/   30] | train loss: 0.0031 | validation loss: 5.1531\n",
      "Epoch [   22/   30] | train loss: 0.0006 | validation loss: 5.0276\n",
      "Epoch [   23/   30] | train loss: 0.0002 | validation loss: 5.3013\n",
      "Epoch [   24/   30] | train loss: 0.0002 | validation loss: 5.5173\n",
      "Epoch [   25/   30] | train loss: 0.0001 | validation loss: 5.6097\n",
      "Epoch [   26/   30] | train loss: 0.0001 | validation loss: 5.4957\n",
      "Epoch [   27/   30] | train loss: 0.0001 | validation loss: 5.5141\n",
      "Epoch [   28/   30] | train loss: 0.0001 | validation loss: 5.6253\n",
      "Epoch [   29/   30] | train loss: 0.0001 | validation loss: 5.6270\n",
      "Epoch [   30/   30] | train loss: 0.0001 | validation loss: 5.5809\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, labels in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # Clear gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model.forward(batch.unsqueeze(1))\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.squeeze(), labels.type(torch.cuda.FloatTensor))\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            validation_loss = 0\n",
    "\n",
    "            for batch, labels in testloader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                ouput = model.forward(batch.unsqueeze(1))\n",
    "                # Calculate loss\n",
    "                loss = criterion(output.squeeze(), labels.type(torch.cuda.FloatTensor))\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = validation_loss / len(testloader)\n",
    "    \n",
    "    training_losses.append(t_loss_avg)\n",
    "    validation_losses.append(v_loss_avg)\n",
    "    \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:6.4f} | validation loss: {:6.4f}'.format(\n",
    "            epoch+1, epochs, t_loss_avg, v_loss_avg))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2017JB015251\n",
    "2. http://scedc.caltech.edu/research-tools/deeplearning.html#picking_polarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
