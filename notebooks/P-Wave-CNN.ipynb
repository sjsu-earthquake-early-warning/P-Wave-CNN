{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=21, padding=10)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=15, padding=7)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=11, padding=5)\n",
    "        \n",
    "        self.batchnorm32 = nn.BatchNorm1d(num_features=32)\n",
    "        self.batchnorm64 = nn.BatchNorm1d(num_features=64)\n",
    "        self.batchnorm128 = nn.BatchNorm1d(num_features=128)\n",
    "        self.batchnorm512 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4736, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm32(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm64(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm128(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten input for fully connected layers\n",
    "        x = x.view(x.shape[0], -1) \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "mixdata = h5py.File(\"../train/scsn_p_2000_2017_6sec_0.5r_pick_train_mix.hdf5\", \"r\")\n",
    "testdata = h5py.File(\"../test/scsn_p_2000_2017_6sec_0.5r_pick_test_mix.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainset(train_val_data, train_val_labels, ratio):\n",
    "    train_ratio = ratio\n",
    "    \n",
    "    trainsize = int(len(train_val_data) * train_ratio)\n",
    "    \n",
    "    trainset = train_val_data[:trainsize]\n",
    "    trainlabels = train_val_labels[:trainsize]\n",
    "    \n",
    "    valset = train_val_data[trainsize:]\n",
    "    valabels = train_val_labels[trainsize:]\n",
    "    \n",
    "    return (trainset, trainlabels), (valset, valabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "train_size = 10000 * 2\n",
    "train_ratio = 0.7\n",
    "test_size = 2000\n",
    "\n",
    "# Load test data\n",
    "train_val_data = mixdata[\"X\"][:train_size]\n",
    "train_val_labels = mixdata[\"pwave\"][:train_size]\n",
    "\n",
    "(trainset, trainlabels), (valset, val_labels) = split_trainset(train_val_data, train_val_labels, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = list(zip(trainset, trainlabels))\n",
    "\n",
    "valset = list(zip(valset, val_labels))\n",
    "\n",
    "testset = testdata[\"X\"][:test_size]\n",
    "testlabels = testdata[\"pwave\"][:test_size]\n",
    "\n",
    "testset = list(zip(testset, testlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Multi-GPU Support to Model \n",
    "In order to run the model on multiple GPU's, we can use the nn.DataParellel method. This method requires that we move all tensors to the cuda:0 (the default gpu) before we can pass them through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(model):\n",
    "    device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CNN(\n",
       "    (conv1): Conv1d(1, 32, kernel_size=(21,), stride=(1,), padding=(10,))\n",
       "    (conv2): Conv1d(32, 64, kernel_size=(15,), stride=(1,), padding=(7,))\n",
       "    (conv3): Conv1d(64, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "    (batchnorm32): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm64): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm128): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchnorm512): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=4736, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (maxpool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parallelize(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function and Optimizer\n",
    "Here we define the loss function and optimizer. For the loss function (criterion), we use the binary cross entropy with logits loss (BCEWithLogitsLoss). This function applies a sigmoid as well as calculates the cross entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/   10] | train loss: 0.0908 | validation loss: 7.9447\n",
      "Epoch [    2/   10] | train loss: 0.0484 | validation loss: 8.9546\n",
      "Epoch [    3/   10] | train loss: 0.0444 | validation loss: 9.4589\n",
      "Epoch [    4/   10] | train loss: 0.0340 | validation loss: 10.3227\n",
      "Epoch [    5/   10] | train loss: 0.0252 | validation loss: 10.9363\n",
      "Epoch [    6/   10] | train loss: 0.0168 | validation loss: 11.4303\n",
      "Epoch [    7/   10] | train loss: 0.0145 | validation loss: 12.5076\n",
      "Epoch [    8/   10] | train loss: 0.0081 | validation loss: 12.8929\n",
      "Epoch [    9/   10] | train loss: 0.0092 | validation loss: 13.5841\n",
      "Epoch [   10/   10] | train loss: 0.0087 | validation loss: 14.2655\n",
      "Wall time: 59.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, labels in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # Clear gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model.forward(batch.unsqueeze(1))\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            for batch, labels in val_loader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                ouput = model.forward(batch.unsqueeze(1))\n",
    "                # Calculate loss\n",
    "                loss = criterion(output, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = val_loss / len(testloader)\n",
    "    \n",
    "    train_losses.append(t_loss_avg)\n",
    "    val_losses.append(v_loss_avg)\n",
    "    \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:6.4f} | validation loss: {:6.4f}'.format(\n",
    "            epoch+1, epochs, t_loss_avg, v_loss_avg))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing Model Performance\n",
    "\n",
    "For testing model performance, we'll be using scikit learn's metrics library. Scikit learn provides a handful of builtin classfication metrics which we can take advantage of. In order to use them with pytorch, we'll have to move the tensors from GPU to CPU and convert them to numpy arrays. \n",
    "\n",
    "1. Classification Report  \n",
    "    a. Precision  \n",
    "    b. Recall  \n",
    "    c. F1 Score\n",
    "2. Accuracy\n",
    "2. AUC-ROC\n",
    "3. AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_to_numpy(tensor):\n",
    "    if tensor.requires_grad:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2017JB015251\n",
    "2. http://scedc.caltech.edu/research-tools/deeplearning.html#picking_polarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
