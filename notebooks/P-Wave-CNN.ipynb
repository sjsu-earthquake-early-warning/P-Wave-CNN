{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Defining the CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=21, padding=10)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=15, padding=7)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=11, padding=5)\n",
    "        \n",
    "        self.batchnorm32 = nn.BatchNorm1d(num_features=32)\n",
    "        self.batchnorm64 = nn.BatchNorm1d(num_features=64)\n",
    "        self.batchnorm128 = nn.BatchNorm1d(num_features=128)\n",
    "        self.batchnorm512 = nn.BatchNorm1d(num_features=512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4736, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm32(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm64(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm128(x))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Flatten input for fully connected layers\n",
    "        x = x.view(x.shape[0], -1) \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(self.batchnorm512(x))\n",
    "        \n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "mixdata = h5py.File(\"../train/scsn_p_2000_2017_6sec_0.5r_pick_train_mix.hdf5\", \"r\")\n",
    "testdata = h5py.File(\"../test/scsn_p_2000_2017_6sec_0.5r_pick_test_mix.hdf5\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainset(train_val_data, train_val_labels, ratio):\n",
    "    train_ratio = ratio\n",
    "    \n",
    "    trainsize = int(len(train_val_data) * train_ratio)\n",
    "    \n",
    "    trainset = train_val_data[:trainsize]\n",
    "    trainlabels = train_val_labels[:trainsize]\n",
    "    \n",
    "    valset = train_val_data[trainsize:]\n",
    "    valabels = train_val_labels[trainsize:]\n",
    "    \n",
    "    return (trainset, trainlabels), (valset, valabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 250\n",
    "\n",
    "train_size = 1 * 10 ** 6\n",
    "train_ratio = 0.7\n",
    "test_size = 1 * 10 ** 5\n",
    "\n",
    "# Load test data\n",
    "train_val_data = mixdata[\"X\"][:train_size]\n",
    "train_val_labels = mixdata[\"pwave\"][:train_size]\n",
    "\n",
    "(trainset, trainlabels), (valset, val_labels) = split_trainset(train_val_data, train_val_labels, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = list(zip(trainset, trainlabels))\n",
    "\n",
    "valset = list(zip(valset, val_labels))\n",
    "\n",
    "testset = testdata[\"X\"][:test_size]\n",
    "testlabels = testdata[\"pwave\"][:test_size]\n",
    "\n",
    "testset = list(zip(testset, testlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Multi-GPU Support to Model \n",
    "In order to run the model on multiple GPU's, we can use the nn.DataParellel method. This method requires that we move all tensors to the cuda:0 (the default gpu) before we can pass them through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(model):\n",
    "    device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = parallelize(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function and Optimizer\n",
    "Here we define the loss function and optimizer. For the loss function (criterion), we use the binary cross entropy with logits loss (BCEWithLogitsLoss). This function applies a sigmoid as well as calculates the cross entropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Validation\n",
    "\n",
    "## TODO -- Save model on validation improvement. Then load model when testing and evaluating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, labels in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # Clear gradients in optimizer\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model.forward(batch.unsqueeze(1))\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "\n",
    "            for batch, labels in val_loader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                ouput = model.forward(batch.unsqueeze(1))\n",
    "                # Calculate loss\n",
    "                loss = criterion(output, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = val_loss / len(val_loader)\n",
    "    \n",
    "    train_losses.append(t_loss_avg)\n",
    "    val_losses.append(v_loss_avg)\n",
    "    \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:6.4f} | validation loss: {:6.4f}'.format(\n",
    "            epoch+1, epochs, t_loss_avg, v_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add learning curve plot\n",
    "train_loss = np.array(train_loss)\n",
    "\n",
    "plt.plot(train_losses, label=\"Training\")\n",
    "plt.plot(val_losses, label=\"Validation\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Testing Model Performance\n",
    "\n",
    "For testing model performance, we'll be using scikit learn's metrics library. Scikit learn provides a handful of builtin classfication metrics which we can take advantage of. In order to use them with pytorch, we'll have to move the tensors from GPU to CPU and convert them to numpy arrays. \n",
    "\n",
    "1. Classification Report  \n",
    "    a. Precision  \n",
    "    b. Recall  \n",
    "    c. F1 Score\n",
    "2. Accuracy\n",
    "2. AUC-ROC\n",
    "3. AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_to_numpy(tensor):\n",
    "    \"\"\"Converts a cuda tensor to a numpy array in place\n",
    "    Positional argument:\n",
    "        tensor -- Tensor to convert to numpy array \n",
    "    \"\"\"\n",
    "    if tensor.requires_grad:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model -- Model to validate with validation data \n",
    "        testloader -- DataLoader with labels and data\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    y_pred = np.array([])\n",
    "    y_true = np.array([])\n",
    "    y_probs = np.array([])\n",
    "    \n",
    "    for batch, labels in testloader:\n",
    "        batch, labels = batch.to(device), labels.to(device)\n",
    "        # forward pass\n",
    "        log_probs = model.forward(batch.unsqueeze(1))\n",
    "        # Calculate class labels\n",
    "        probs = torch.exp(log_probs)\n",
    "        top_p, top_class = probs.topk(1, dim=1)\n",
    "        \n",
    "        y_pred = np.append(y_pred, cuda_to_numpy(top_class))\n",
    "        y_true = np.append(y_true, cuda_to_numpy(labels))\n",
    "        y_probs = np.append(y_probs, cuda_to_numpy(top_p))\n",
    "        \n",
    "    target_names = [\"p_wave\", \"noise\"]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print(\"Accuracy: {}%\".format(accuracy_score(y_true, y_pred) * 100))\n",
    "    print(\"AUC-ROC Score: {}\".format(roc_auc_score(y_true, y_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_model(model, testloader, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2017JB015251\n",
    "2. http://scedc.caltech.edu/research-tools/deeplearning.html#picking_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
