{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import mlflow\n",
    "\n",
    "import scipy.fftpack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from lib.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the Resnet18 Architecture\n",
    "Only change the last layer to a Fully Connected Layer with a LogSoftmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(in_features=512, out_features=2, bias=True)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "model.fc = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load\n",
    "import h5py\n",
    "from lib.utils import split_trainset\n",
    "\n",
    "mixdata = h5py.File(\"../train/scsn_p_2000_2017_6sec_0.5r_pick_train_mix.hdf5\", \"r\")\n",
    "testdata = h5py.File(\"../test/scsn_p_2000_2017_6sec_0.5r_pick_test_mix.hdf5\", \"r\")\n",
    "\n",
    "batch_size = 50\n",
    "train_size = 1 * 10 ** 4\n",
    "train_ratio = 0.7\n",
    "test_size = 1 * 10 ** 4\n",
    "\n",
    "train_val_data = mixdata[\"X\"][:train_size]\n",
    "train_val_labels = mixdata[\"pwave\"][:train_size]\n",
    "\n",
    "trainset, valset = split_trainset(train_val_data, train_val_labels, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "from lib.utils import Waveform2Spectrogram, ResnetSpectrogramDataset\n",
    "\n",
    "t = transforms.Compose([\n",
    "  Waveform2Spectrogram(),\n",
    "  transforms.Resize((224, 224)),\n",
    "  transforms.Grayscale(num_output_channels=3),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                       [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trainset = ResnetSpectrogramDataset(trainset, transform=t)\n",
    "valset = ResnetSpectrogramDataset(valset, transform=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Add Multi-GPU Support to Model \n",
    "In order to run the model on multiple GPU's, we can use the nn.DataParallel layer. This layer requires that we move all tensors to the cuda:0 (the default gpu) before we can pass them through the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(model):\n",
    "    device_ids = [i for i in range(torch.cuda.device_count())]\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Sequential(\n",
       "    (fc1): Linear(in_features=512, out_features=2, bias=True)\n",
       "    (output): LogSoftmax()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if (device == \"cuda\"):\n",
    "#    model = parallelize(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Loss Function and Optimizer\n",
    "TODO: Describe what loss function and optimizer to use and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training and Validation\n",
    "\n",
    "## TODO -- Save model on validation improvement. Then load model when testing and evaluating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ActiveRun: info=<RunInfo: run_uuid='52d4999ead034df3a7e4741c9e5f016a', experiment_id=0, name='', source_type=4, source_name='C:\\\\Users\\\\rober\\\\Miniconda3\\\\envs\\\\eew\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', entry_point_name=None, user_id='unknown', status=1, start_time=1553739887491, end_time=None, source_version=None, lifecycle_stage='active', artifact_uri='.\\\\mlruns\\\\0\\\\52d4999ead034df3a7e4741c9e5f016a\\\\artifacts'>, data=None>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"file:.\\mlruns\")\n",
    "mlflow.start_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/   10] | train loss: 0.4380 | validation loss: 0.5490 | validation accuracy: 76.8000%\n",
      "Epoch [    2/   10] | train loss: 0.3671 | validation loss: 0.3607 | validation accuracy: 84.6000%\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, labels in trainloader:\n",
    "        # ============================================\n",
    "        #            TRAINING\n",
    "        # ============================================\n",
    "        if (device == \"cuda\"):\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "        # Forward pass\n",
    "        output = model.forward(batch)\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Calculate loss\n",
    "        if (device == \"cuda\"):\n",
    "            loss = criterion(output, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "        else:\n",
    "            loss = criterion(output, labels.type(torch.LongTensor).view(labels.shape, 1))\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            y_pred = np.array([])\n",
    "            y_true = np.array([])\n",
    "            \n",
    "            for batch, labels in val_loader:\n",
    "                # ============================================\n",
    "                #            VALIDATION\n",
    "                # ============================================\n",
    "                if (device == \"cuda\"):\n",
    "                    batch, labels = batch.to(device), labels.to(device)\n",
    "                # Forward pass\n",
    "                log_probs = model.forward(batch)\n",
    "                probs = torch.exp(log_probs)\n",
    "                \n",
    "                top_p, top_class = probs.topk(1, dim=1)\n",
    "                y_pred = np.append(y_pred, cuda_to_numpy(top_class))\n",
    "                y_true = np.append(y_true, cuda_to_numpy(labels))\n",
    "                # Calculate loss\n",
    "                if (device == \"cuda\"):\n",
    "                    loss = criterion(log_probs, labels.type(torch.cuda.LongTensor).view(labels.shape, 1))\n",
    "                else:\n",
    "                    loss = criterion(log_probss, labels.type(torch.LongTensor).view(labels.shape, 1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "    # Print epoch summary\n",
    "    t_loss_avg = train_loss / len(trainloader)\n",
    "    v_loss_avg = val_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    \n",
    "    mlflow.log_metric(\"train_loss\", t_loss_avg)\n",
    "    mlflow.log_metric(\"val_loss\", v_loss_avg)\n",
    "    mlflow.log_metric(\"val_accuracy\", accuracy)\n",
    "\n",
    "    \n",
    "    train_losses.append(t_loss_avg)\n",
    "    val_losses.append(v_loss_avg)\n",
    "    val_accuracies.append(accuracy)\n",
    "\n",
    "    \n",
    "    print('Epoch [{:5d}/{:5d}] | train loss: {:6.4f} | validation loss: {:6.4f} | validation accuracy: {:6.4f}%'.format(\n",
    "            epoch+1, epochs, t_loss_avg, v_loss_avg, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "One of the challenges of modifying the architecture of an existing CNN is to accomodate for the first layer's input tensor shape.\n",
    "\n",
    "For this problem specifically, **how do we translate time-series data of shape (300,) into a (224, 224, 3) tensor?**\n",
    "\n",
    "Briefly, what I did was obtain a spectrogram of the waveform by applying DFT to overlapping windows of the data. This gives us a heatmap of which frequencies had a stronger presence at some specific time. Then, I treat the spectrogram as an image and upscale it to the required dimensions, (224, 224). Finally, I treat the upscaled spectrogram as a greyscale image to give it the 3 channels necessary to work with Resnet.\n",
    "\n",
    "Below is an elaboration of the process outlined above, with a noise waveform and p-wave waveform used as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First, let's load the data and compare the two types of waveforms.\n",
    "\"\"\"\n",
    "file = h5py.File(train_filepath, 'r')\n",
    "dataset = file.get(features_key)[:dataset_size]\n",
    "file.close()\n",
    "\n",
    "noise = dataset[0]\n",
    "pwave = dataset[1]\n",
    "\n",
    "plt.title('Noise(R) vs P-wave(B)')\n",
    "plt.plot(range(len(noise)), noise, 'r.-')\n",
    "plt.plot(range(len(pwave)), pwave, 'b.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Then, let's take a look at the Fast Fourier Transform (just the real part) of the waveforms.\n",
    "We actually don't need this to calculate the spectrograms, but it's here to get a better\n",
    "understanding of how the waveforms can be interpreted.\n",
    "\"\"\"\n",
    "noise_fft = list(map(lambda x: 0 if x < 0 else x, scipy.fftpack.rfft(noise)))\n",
    "pwave_fft = list(map(lambda x: 0 if x < 0 else x, scipy.fftpack.rfft(pwave)))\n",
    "t = np.linspace(0, 3, 300)\n",
    "\n",
    "plt.title(\"FFT Noise\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.plot(t, noise_fft)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"FFT P-Wave\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Frequency (Hz)\")\n",
    "plt.plot(t, pwave_fft)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, let's get the spectrograms using scipy.signal.spectrogram\n",
    "\"\"\"\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html\n",
    "from scipy.signal import spectrogram\n",
    "\n",
    "noise_freqs, noise_times, noise_Sx = spectrogram(noise, fs=100, window='hamming',\n",
    "                                                nperseg=30, noverlap=0,\n",
    "                                                detrend='linear', scaling='spectrum')\n",
    "pwave_freqs, pwave_times, pwave_Sx = spectrogram(pwave, fs=100, window='hamming',\n",
    "                                                nperseg=30, noverlap=0,\n",
    "                                                detrend='linear', scaling='spectrum')\n",
    "\n",
    "plt.pcolormesh(noise_times, noise_freqs, 10 * np.log10(noise_Sx), cmap='viridis')\n",
    "plt.title('Noise Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]');\n",
    "plt.show()\n",
    "\n",
    "plt.title('P-Wave Spectrogram')\n",
    "plt.pcolormesh(pwave_times, pwave_freqs, 10 * np.log10(pwave_Sx), cmap='viridis')\n",
    "plt.title('P-Wave Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can already see how Resnet can accept these spectograms as images, but the\n",
    "spectograms shape isn't quite right. Right now, it's (16, 10) and is determined\n",
    "by the arguments passed to scipy.signal.spectrogram. Either way, we can resize\n",
    "it to (224, 224) using torchvision.transforms.Grayscale (skimage.transform.resize works too).\n",
    "\"\"\"\n",
    "# https://scikit-image.org/docs/dev/api/skimage.transform.html?highlight=resize#skimage.transform.resize\n",
    "from skimage.transform import resize\n",
    "\n",
    "noise_Sx = resize(noise_Sx, (224, 224), mode='reflect')\n",
    "pwave_Sx = resize(pwave_Sx, (224, 224), mode='reflect')\n",
    "\n",
    "# In order to correctly print the spectrogram, we must also change the time and\n",
    "# frequency lists to reflect the resized spectrograms.\n",
    "noise_times = np.linspace(noise_times[0], noise_times[-1], 224)\n",
    "noise_freqs = np.linspace(noise_freqs[0], noise_freqs[-1], 224)\n",
    "pwave_times = np.linspace(pwave_times[0], pwave_times[-1], 224)\n",
    "pwave_freqs = np.linspace(pwave_freqs[0], pwave_freqs[-1], 224)\n",
    "\n",
    "plt.pcolormesh(noise_times, noise_freqs, 10 * np.log10(noise_Sx), cmap='viridis')\n",
    "plt.title('Noise Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]');\n",
    "plt.show()\n",
    "\n",
    "plt.title('P-Wave Spectrogram')\n",
    "plt.pcolormesh(pwave_times, pwave_freqs, 10 * np.log10(pwave_Sx), cmap='viridis')\n",
    "plt.title('P-Wave Spectrogram')\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Getting closer now! The spectogram's shapes are (224, 224), but we're still missing\n",
    "the three channels (RGB). So, let's treat the spectrograms like greyscale images to derive\n",
    "the other channels. We can do this using skimage.color.gray2rgb (either spelling of gray works!).\n",
    "\"\"\"\n",
    "# http://scikit-image.org/docs/dev/user_guide/viewer.html\n",
    "from skimage.color import gray2rgb\n",
    "noise_Sx = gray2rgb(noise_Sx)\n",
    "pwave_Sx = gray2rgb(pwave_Sx)\n",
    "print(f\"Noise Spectrogram Shape: {noise_Sx.shape}\\nP-Wave Spectrogram Shape: {pwave_Sx.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def imshow(inp, title=None):\n",
    "#     \"\"\"Imshow for Tensor.\"\"\"\n",
    "#     inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "#     inp = np.clip(inp, 0, 1)\n",
    "#     plt.imshow(inp)\n",
    "#     if title is not None:\n",
    "#         plt.title(title)\n",
    "#     plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# # Get a batch of training data\n",
    "# sample = next(iter(train_dataloader))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(sample[\"waveform\"])\n",
    "\n",
    "# imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
